{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e4a9d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import pickle\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0d2ef5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 128613 entries, 0 to 128612\n",
      "Data columns (total 5 columns):\n",
      "Speaker_Bioguide_ID    128613 non-null object\n",
      "Speaker_Name           128613 non-null object\n",
      "Text                   128613 non-null object\n",
      "Date                   128613 non-null object\n",
      "Legislative Body       128613 non-null object\n",
      "dtypes: object(5)\n",
      "memory usage: 4.9+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# raw_data = pd.read_csv('combined_speakers_ids_and_speech_texts_2017_2021.csv')\n",
    "# print(raw_data.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "980febfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 85977 entries, 0 to 128575\n",
      "Data columns (total 5 columns):\n",
      "Speaker_Bioguide_ID    85977 non-null object\n",
      "Speaker_Name           85977 non-null object\n",
      "Text                   85977 non-null object\n",
      "Date                   85977 non-null object\n",
      "Legislative Body       85977 non-null object\n",
      "dtypes: object(5)\n",
      "memory usage: 3.9+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# raw_data = raw_data[raw_data['Legislative Body']=='House']\n",
    "# print(raw_data.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fa974b6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 85977 entries, 0 to 128575\n",
      "Data columns (total 5 columns):\n",
      "Speaker_Bioguide_ID    85977 non-null object\n",
      "Speaker_Name           85977 non-null object\n",
      "Text                   85977 non-null object\n",
      "Date                   85977 non-null object\n",
      "Legislative Body       85977 non-null object\n",
      "dtypes: object(5)\n",
      "memory usage: 3.9+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# raw_data['Text'] = raw_data['Text'].apply(lambda x: x[2:])\n",
    "# print(raw_data.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4eaa9645",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 85977 entries, 0 to 128575\n",
      "Data columns (total 5 columns):\n",
      "Speaker_Bioguide_ID    85977 non-null object\n",
      "Speaker_Name           85977 non-null object\n",
      "Text                   85977 non-null object\n",
      "Date                   85977 non-null object\n",
      "Legislative Body       85977 non-null object\n",
      "dtypes: object(5)\n",
      "memory usage: 3.9+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# raw_data['Text'] = raw_data['Text'].apply(lambda x: x[2:])\n",
    "# print(raw_data.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "78745629",
   "metadata": {},
   "outputs": [],
   "source": [
    "#raw_data.to_csv('raw_original_data_floor_speeches_house.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "6364998e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 85977 entries, 0 to 85976\n",
      "Data columns (total 5 columns):\n",
      "Speaker_Bioguide_ID    85977 non-null object\n",
      "Speaker_Name           85977 non-null object\n",
      "Text                   85977 non-null object\n",
      "Date                   85977 non-null object\n",
      "Legislative Body       85977 non-null object\n",
      "dtypes: object(5)\n",
      "memory usage: 3.3+ MB\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Speaker_Bioguide_ID</th>\n",
       "      <th>Speaker_Name</th>\n",
       "      <th>Text</th>\n",
       "      <th>Date</th>\n",
       "      <th>Legislative Body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>M001201</td>\n",
       "      <td>Mr. MITCHELL</td>\n",
       "      <td>Mr. Speaker, I rise today in the spirit of Mad...</td>\n",
       "      <td>2017-07-20</td>\n",
       "      <td>House</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B001250</td>\n",
       "      <td>Mr. BISHOP of Utah</td>\n",
       "      <td>Mr. Speaker, I ask unanimous consent that all ...</td>\n",
       "      <td>2017-07-20</td>\n",
       "      <td>House</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B001250</td>\n",
       "      <td>Mr. BISHOP of Utah</td>\n",
       "      <td>Mr. Chair, I include in the Record my statemen...</td>\n",
       "      <td>2017-07-20</td>\n",
       "      <td>House</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B001250</td>\n",
       "      <td>Mr. BISHOP of Utah</td>\n",
       "      <td>Mr. Chair, I yield 5 minutes to the gentleman ...</td>\n",
       "      <td>2017-07-20</td>\n",
       "      <td>House</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Y000033</td>\n",
       "      <td>Mr. YOUNG of Alaska</td>\n",
       "      <td>Mr. Chairman, this is an issue that should hav...</td>\n",
       "      <td>2017-07-20</td>\n",
       "      <td>House</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Speaker_Bioguide_ID         Speaker_Name  \\\n",
       "0             M001201         Mr. MITCHELL   \n",
       "1             B001250   Mr. BISHOP of Utah   \n",
       "2             B001250   Mr. BISHOP of Utah   \n",
       "3             B001250   Mr. BISHOP of Utah   \n",
       "4             Y000033  Mr. YOUNG of Alaska   \n",
       "\n",
       "                                                Text        Date  \\\n",
       "0  Mr. Speaker, I rise today in the spirit of Mad...  2017-07-20   \n",
       "1  Mr. Speaker, I ask unanimous consent that all ...  2017-07-20   \n",
       "2  Mr. Chair, I include in the Record my statemen...  2017-07-20   \n",
       "3  Mr. Chair, I yield 5 minutes to the gentleman ...  2017-07-20   \n",
       "4  Mr. Chairman, this is an issue that should hav...  2017-07-20   \n",
       "\n",
       "  Legislative Body  \n",
       "0            House  \n",
       "1            House  \n",
       "2            House  \n",
       "3            House  \n",
       "4            House  "
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data = pd.read_csv('raw_original_data_floor_speeches_house.csv')\n",
    "print(raw_data.info())\n",
    "raw_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c0b5d9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "573"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(raw_data['Speaker_Bioguide_ID']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6bfcd430",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. we must have bio info on all the speakers\n",
    "legis_info = json.load(open('legislator-info-1990-2020.json'))\n",
    "legis_id_to_info = {}\n",
    "for x in legis_info:\n",
    "    legis_id_to_info[x['id']['bioguide']] = x\n",
    "del legis_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "3990cb36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "speakers_to_remove_based_on_non_availbility_of_bio_info = set() \n",
    "speakers = list(raw_data['Speaker_Bioguide_ID'])\n",
    "for s in speakers:\n",
    "    if s not in legis_id_to_info:\n",
    "        speakers_to_remove_based_on_non_availbility_of_bio_info.add(s)\n",
    "print(len(speakers_to_remove_based_on_non_availbility_of_bio_info))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "9ebdfbf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 85733 entries, 0 to 85976\n",
      "Data columns (total 5 columns):\n",
      "Speaker_Bioguide_ID    85733 non-null object\n",
      "Speaker_Name           85733 non-null object\n",
      "Text                   85733 non-null object\n",
      "Date                   85733 non-null object\n",
      "Legislative Body       85733 non-null object\n",
      "dtypes: object(5)\n",
      "memory usage: 3.9+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "raw_data = raw_data[~raw_data['Speaker_Bioguide_ID'].isin(speakers_to_remove_based_on_non_availbility_of_bio_info)]\n",
    "print(raw_data.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "1f3f0e0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59\n"
     ]
    }
   ],
   "source": [
    "#remove speakers if they gave less than 25 speeches - TBIP paper removed senators with less than 24 speeches.\n",
    "speakers_to_remove_based_on_num_speeches = set()\n",
    "speakers = set(raw_data['Speaker_Bioguide_ID'])\n",
    "thresh = 25\n",
    "for s in speakers:\n",
    "    n_s = len(raw_data[raw_data['Speaker_Bioguide_ID']==s])\n",
    "    if n_s < thresh:\n",
    "        speakers_to_remove_based_on_num_speeches.add(s)\n",
    "print(len(speakers_to_remove_based_on_num_speeches))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "33632a0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 85173 entries, 0 to 85976\n",
      "Data columns (total 5 columns):\n",
      "Speaker_Bioguide_ID    85173 non-null object\n",
      "Speaker_Name           85173 non-null object\n",
      "Text                   85173 non-null object\n",
      "Date                   85173 non-null object\n",
      "Legislative Body       85173 non-null object\n",
      "dtypes: object(5)\n",
      "memory usage: 3.9+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "raw_data = raw_data[~raw_data['Speaker_Bioguide_ID'].isin(speakers_to_remove_based_on_num_speeches)]\n",
    "print(raw_data.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8988abcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "92b5292e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "555\n"
     ]
    }
   ],
   "source": [
    "names_in_cong_record = list(raw_data['Speaker_Name'])\n",
    "for n in names_in_cong_record:\n",
    "    l = n.split()\n",
    "    for x in l:\n",
    "        stopwords.add(x.lower())\n",
    "print(len(stopwords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b29e8d29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'abraham',\n",
       " 'adams',\n",
       " 'aderholt',\n",
       " 'aguilar',\n",
       " 'al',\n",
       " 'alabama',\n",
       " 'alaska',\n",
       " 'allen',\n",
       " 'amash',\n",
       " 'arizona',\n",
       " 'arkansas',\n",
       " 'armstrong',\n",
       " 'arrington',\n",
       " 'austin',\n",
       " 'axne',\n",
       " 'b.',\n",
       " 'babin',\n",
       " 'bacon',\n",
       " 'baird',\n",
       " 'banks',\n",
       " 'barletta',\n",
       " 'barr',\n",
       " 'barragan',\n",
       " 'barton',\n",
       " 'bass',\n",
       " 'beatty',\n",
       " 'ben',\n",
       " 'bera',\n",
       " 'bergman',\n",
       " 'bernice',\n",
       " 'beutler',\n",
       " 'beyer',\n",
       " 'biggs',\n",
       " 'bilirakis',\n",
       " 'bishop',\n",
       " 'black',\n",
       " 'blackburn',\n",
       " 'blum',\n",
       " 'blumenauer',\n",
       " 'blunt',\n",
       " 'bonamici',\n",
       " 'bordallo',\n",
       " 'bost',\n",
       " 'boyle',\n",
       " 'brady',\n",
       " 'brendan',\n",
       " 'brindisi',\n",
       " 'brooks',\n",
       " 'brown',\n",
       " 'brownley',\n",
       " 'buck',\n",
       " 'bucshon',\n",
       " 'budd',\n",
       " 'burchett',\n",
       " 'burgess',\n",
       " 'bustos',\n",
       " 'butterfield',\n",
       " 'byrne',\n",
       " 'california',\n",
       " 'calvert',\n",
       " 'capuano',\n",
       " 'carbajal',\n",
       " 'cardenas',\n",
       " 'carolina',\n",
       " 'carolyn',\n",
       " 'carson',\n",
       " 'carter',\n",
       " 'cartwright',\n",
       " 'case',\n",
       " 'casten',\n",
       " 'castor',\n",
       " 'castro',\n",
       " 'chabot',\n",
       " 'chaffetz',\n",
       " 'cheney',\n",
       " 'chu',\n",
       " 'cicilline',\n",
       " 'cisneros',\n",
       " 'clark',\n",
       " 'clarke',\n",
       " 'clay',\n",
       " 'cleaver',\n",
       " 'cline',\n",
       " 'cloud',\n",
       " 'clyburn',\n",
       " 'coffman',\n",
       " 'cohen',\n",
       " 'cole',\n",
       " 'coleman',\n",
       " 'collins',\n",
       " 'colorado',\n",
       " 'comer',\n",
       " 'comstock',\n",
       " 'conaway',\n",
       " 'connecticut',\n",
       " 'connolly',\n",
       " 'conyers',\n",
       " 'cook',\n",
       " 'correa',\n",
       " 'costa',\n",
       " 'costello',\n",
       " 'courtney',\n",
       " 'cox',\n",
       " 'craig',\n",
       " 'crawford',\n",
       " 'crenshaw',\n",
       " 'crist',\n",
       " 'crow',\n",
       " 'crowley',\n",
       " 'cuellar',\n",
       " 'culberson',\n",
       " 'cummings',\n",
       " 'cunningham',\n",
       " 'curbelo',\n",
       " 'curtis',\n",
       " 'dakota',\n",
       " 'danny',\n",
       " 'david',\n",
       " 'davidson',\n",
       " 'davis',\n",
       " 'dean',\n",
       " 'defazio',\n",
       " 'degette',\n",
       " 'delauro',\n",
       " 'delbene',\n",
       " 'delgado',\n",
       " 'demings',\n",
       " 'denham',\n",
       " 'desantis',\n",
       " 'desaulnier',\n",
       " 'desjarlais',\n",
       " 'deutch',\n",
       " 'diaz-balart',\n",
       " 'dingell',\n",
       " 'doggett',\n",
       " 'donovan',\n",
       " 'doyle',\n",
       " 'drew',\n",
       " 'duffy',\n",
       " 'duncan',\n",
       " 'dunn',\n",
       " 'eddie',\n",
       " 'ellison',\n",
       " 'emmer',\n",
       " 'engel',\n",
       " 'escobar',\n",
       " 'eshoo',\n",
       " 'espaillat',\n",
       " 'estes',\n",
       " 'esty',\n",
       " 'evans',\n",
       " 'f.',\n",
       " 'farenthold',\n",
       " 'faso',\n",
       " 'ferguson',\n",
       " 'fitzpatrick',\n",
       " 'fleischmann',\n",
       " 'fletcher',\n",
       " 'flores',\n",
       " 'florida',\n",
       " 'fortenberry',\n",
       " 'foster',\n",
       " 'foxx',\n",
       " 'francis',\n",
       " 'frankel',\n",
       " 'franks',\n",
       " 'frelinghuysen',\n",
       " 'fudge',\n",
       " 'fulcher',\n",
       " 'gabbard',\n",
       " 'gaetz',\n",
       " 'gallagher',\n",
       " 'gallego',\n",
       " 'garamendi',\n",
       " 'garcia',\n",
       " 'garrett',\n",
       " 'gene',\n",
       " 'georgia',\n",
       " 'gianforte',\n",
       " 'gibbs',\n",
       " 'gohmert',\n",
       " 'golden',\n",
       " 'gomez',\n",
       " 'gonzalez',\n",
       " 'gonzalez-colon',\n",
       " 'goodlatte',\n",
       " 'gosar',\n",
       " 'gottheimer',\n",
       " 'granger',\n",
       " 'graves',\n",
       " 'green',\n",
       " 'griffith',\n",
       " 'grijalva',\n",
       " 'grisham',\n",
       " 'grothman',\n",
       " 'guest',\n",
       " 'guthrie',\n",
       " 'gutierrez',\n",
       " 'haaland',\n",
       " 'hampshire',\n",
       " 'hanabusa',\n",
       " 'handel',\n",
       " 'harder',\n",
       " 'harper',\n",
       " 'harris',\n",
       " 'hartzler',\n",
       " 'hastings',\n",
       " 'hayes',\n",
       " 'heck',\n",
       " 'hensarling',\n",
       " 'hern',\n",
       " 'herrera',\n",
       " 'hice',\n",
       " 'higgins',\n",
       " 'hill',\n",
       " 'himes',\n",
       " 'holding',\n",
       " 'hollingsworth',\n",
       " 'horn',\n",
       " 'horsford',\n",
       " 'houlahan',\n",
       " 'hoyer',\n",
       " 'hudson',\n",
       " 'huffman',\n",
       " 'huizenga',\n",
       " 'hultgren',\n",
       " 'hunter',\n",
       " 'hurd',\n",
       " 'illinois',\n",
       " 'indiana',\n",
       " 'iowa',\n",
       " 'issa',\n",
       " 'jackson',\n",
       " 'jackson-lee',\n",
       " 'jayapal',\n",
       " 'jeffries',\n",
       " 'jenkins',\n",
       " 'jersey',\n",
       " 'jody',\n",
       " 'john',\n",
       " 'johnson',\n",
       " 'jordan',\n",
       " 'joyce',\n",
       " 'judy',\n",
       " 'k.',\n",
       " 'kansas',\n",
       " 'kaptur',\n",
       " 'katko',\n",
       " 'keating',\n",
       " 'keller',\n",
       " 'kelly',\n",
       " 'kendra',\n",
       " 'kennedy',\n",
       " 'kentucky',\n",
       " 'kevin',\n",
       " 'khanna',\n",
       " 'kihuen',\n",
       " 'kildee',\n",
       " 'kilmer',\n",
       " 'kim',\n",
       " 'kind',\n",
       " 'king',\n",
       " 'kinzinger',\n",
       " 'kirkpatrick',\n",
       " 'knight',\n",
       " 'krishnamoorthi',\n",
       " 'kuster',\n",
       " 'kustoff',\n",
       " 'lahood',\n",
       " 'lamalfa',\n",
       " 'lamb',\n",
       " 'lamborn',\n",
       " 'lance',\n",
       " 'langevin',\n",
       " 'larsen',\n",
       " 'larson',\n",
       " 'latta',\n",
       " 'lawrence',\n",
       " 'lawson',\n",
       " 'lee',\n",
       " 'lesko',\n",
       " 'levin',\n",
       " 'lewis',\n",
       " 'lieu',\n",
       " 'lipinski',\n",
       " 'lofgren',\n",
       " 'loudermilk',\n",
       " 'louisiana',\n",
       " 'love',\n",
       " 'lowenthal',\n",
       " 'lowey',\n",
       " 'lucas',\n",
       " 'luetkemeyer',\n",
       " 'lujan',\n",
       " 'luria',\n",
       " 'lynch',\n",
       " 'maine',\n",
       " 'malinowski',\n",
       " 'maloney',\n",
       " 'marchant',\n",
       " 'marino',\n",
       " 'marshall',\n",
       " 'maryland',\n",
       " 'massachusetts',\n",
       " 'massie',\n",
       " 'mast',\n",
       " 'matsui',\n",
       " 'maxine',\n",
       " 'mcadams',\n",
       " 'mcbath',\n",
       " 'mccarthy',\n",
       " 'mccaul',\n",
       " 'mcclintock',\n",
       " 'mccollum',\n",
       " 'mceachin',\n",
       " 'mcgovern',\n",
       " 'mchenry',\n",
       " 'mckinley',\n",
       " 'mcnerney',\n",
       " 'mcsally',\n",
       " 'meadows',\n",
       " 'meeks',\n",
       " 'meng',\n",
       " 'messer',\n",
       " 'meuser',\n",
       " 'mexico',\n",
       " 'michael',\n",
       " 'michelle',\n",
       " 'michigan',\n",
       " 'miller',\n",
       " 'mimi',\n",
       " 'minnesota',\n",
       " 'miss',\n",
       " 'mississippi',\n",
       " 'missouri',\n",
       " 'mitchell',\n",
       " 'mooney',\n",
       " 'moore',\n",
       " 'morelle',\n",
       " 'moulton',\n",
       " 'mr',\n",
       " 'mr.',\n",
       " 'mrs.',\n",
       " 'ms',\n",
       " 'ms.',\n",
       " 'mucarsel-powell',\n",
       " 'mullin',\n",
       " 'murphy',\n",
       " 'nadler',\n",
       " 'napolitano',\n",
       " 'neal',\n",
       " 'nebraska',\n",
       " 'neguse',\n",
       " 'nevada',\n",
       " 'new',\n",
       " 'newhouse',\n",
       " 'nicolas',\n",
       " 'noem',\n",
       " 'nolan',\n",
       " 'norcross',\n",
       " 'norman',\n",
       " 'north',\n",
       " 'norton',\n",
       " 'nunes',\n",
       " \"o'halleran\",\n",
       " \"o'rourke\",\n",
       " 'ocasio-cortez',\n",
       " 'of',\n",
       " 'ohio',\n",
       " 'oklahoma',\n",
       " 'olson',\n",
       " 'omar',\n",
       " 'p.',\n",
       " 'palazzo',\n",
       " 'pallone',\n",
       " 'palmer',\n",
       " 'panetta',\n",
       " 'pappas',\n",
       " 'pascrell',\n",
       " 'patrick',\n",
       " 'paulsen',\n",
       " 'payne',\n",
       " 'pearce',\n",
       " 'pelosi',\n",
       " 'pence',\n",
       " 'pennsylvania',\n",
       " 'perlmutter',\n",
       " 'perry',\n",
       " 'peters',\n",
       " 'peterson',\n",
       " 'phillips',\n",
       " 'pingree',\n",
       " 'pittenger',\n",
       " 'plaskett',\n",
       " 'pocan',\n",
       " 'poe',\n",
       " 'poliquin',\n",
       " 'polis',\n",
       " 'porter',\n",
       " 'posey',\n",
       " 'pressley',\n",
       " 'price',\n",
       " 'puerto',\n",
       " 'quigley',\n",
       " 'radewagen',\n",
       " 'raskin',\n",
       " 'ratcliffe',\n",
       " 'ray',\n",
       " 'reed',\n",
       " 'reichert',\n",
       " 'reschenthaler',\n",
       " 'rice',\n",
       " 'richmond',\n",
       " 'rico',\n",
       " 'riggleman',\n",
       " 'roby',\n",
       " 'rochester',\n",
       " 'rodgers',\n",
       " 'rodney',\n",
       " 'roe',\n",
       " 'rogers',\n",
       " 'rohrabacher',\n",
       " 'rokita',\n",
       " 'rooney',\n",
       " 'ros-lehtinen',\n",
       " 'rose',\n",
       " 'rosen',\n",
       " 'roskam',\n",
       " 'ross',\n",
       " 'rothfus',\n",
       " 'rouda',\n",
       " 'rouzer',\n",
       " 'roy',\n",
       " 'roybal-allard',\n",
       " 'royce',\n",
       " 'ruiz',\n",
       " 'rush',\n",
       " 'russell',\n",
       " 'rutherford',\n",
       " 'ryan',\n",
       " 's.',\n",
       " 'sablan',\n",
       " 'sam',\n",
       " 'san',\n",
       " 'sanchez',\n",
       " 'sanford',\n",
       " 'sarbanes',\n",
       " 'scalise',\n",
       " 'scanlon',\n",
       " 'schakowsky',\n",
       " 'schiff',\n",
       " 'schneider',\n",
       " 'schrader',\n",
       " 'schrier',\n",
       " 'schultz',\n",
       " 'schweikert',\n",
       " 'scott',\n",
       " 'sean',\n",
       " 'sensenbrenner',\n",
       " 'serrano',\n",
       " 'sessions',\n",
       " 'sewell',\n",
       " 'shalala',\n",
       " 'shea-porter',\n",
       " 'sherman',\n",
       " 'sherrill',\n",
       " 'shimkus',\n",
       " 'shuster',\n",
       " 'simpson',\n",
       " 'sinema',\n",
       " 'sires',\n",
       " 'slaughter',\n",
       " 'slotkin',\n",
       " 'small',\n",
       " 'smith',\n",
       " 'smucker',\n",
       " 'soto',\n",
       " 'south',\n",
       " 'spanberger',\n",
       " 'spano',\n",
       " 'speier',\n",
       " 'stanton',\n",
       " 'stauber',\n",
       " 'stefanik',\n",
       " 'steil',\n",
       " 'steube',\n",
       " 'stevens',\n",
       " 'stewart',\n",
       " 'stivers',\n",
       " 'suozzi',\n",
       " 'swalwell',\n",
       " 'takano',\n",
       " 'taylor',\n",
       " 'ted',\n",
       " 'tennessee',\n",
       " 'tenney',\n",
       " 'texas',\n",
       " 'thompson',\n",
       " 'thornberry',\n",
       " 'tiberi',\n",
       " 'tipton',\n",
       " 'titus',\n",
       " 'tlaib',\n",
       " 'tonko',\n",
       " 'torres',\n",
       " 'trahan',\n",
       " 'trone',\n",
       " 'trott',\n",
       " 'tsongas',\n",
       " 'turner',\n",
       " 'underwood',\n",
       " 'upton',\n",
       " 'utah',\n",
       " 'van',\n",
       " 'vargas',\n",
       " 'veasey',\n",
       " 'vela',\n",
       " 'velazquez',\n",
       " 'virginia',\n",
       " 'visclosky',\n",
       " 'w.',\n",
       " 'wagner',\n",
       " 'walberg',\n",
       " 'walden',\n",
       " 'walker',\n",
       " 'walorski',\n",
       " 'walters',\n",
       " 'waltz',\n",
       " 'walz',\n",
       " 'washington',\n",
       " 'wasserman',\n",
       " 'waters',\n",
       " 'watkins',\n",
       " 'watson',\n",
       " 'weber',\n",
       " 'webster',\n",
       " 'welch',\n",
       " 'wenstrup',\n",
       " 'west',\n",
       " 'westerman',\n",
       " 'wexton',\n",
       " 'wild',\n",
       " 'williams',\n",
       " 'wilson',\n",
       " 'wisconsin',\n",
       " 'wittman',\n",
       " 'womack',\n",
       " 'woodall',\n",
       " 'wright',\n",
       " 'yarmuth',\n",
       " 'yoder',\n",
       " 'yoho',\n",
       " 'york',\n",
       " 'young',\n",
       " 'zeldin'}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "68acd911",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "895\n"
     ]
    }
   ],
   "source": [
    "bioguide_ids = set(raw_data['Speaker_Bioguide_ID'])\n",
    "for bid in bioguide_ids:\n",
    "    name = list(legis_id_to_info[bid]['name'].values())\n",
    "    for x in name:\n",
    "        for z in x.split():\n",
    "            stopwords.add(z.lower().replace('\"', '').replace(\"'\", ''))\n",
    "print(len(stopwords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0d572c60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a.',\n",
       " 'abigail',\n",
       " 'abney',\n",
       " 'abraham',\n",
       " 'adam',\n",
       " 'adams',\n",
       " 'aderholt',\n",
       " 'adrian',\n",
       " 'adriano',\n",
       " 'aguilar',\n",
       " 'al',\n",
       " 'alabama',\n",
       " 'alan',\n",
       " 'alaska',\n",
       " 'albio',\n",
       " 'alcee',\n",
       " 'alex',\n",
       " 'alexander',\n",
       " 'alexandria',\n",
       " 'allen',\n",
       " 'alma',\n",
       " 'amash',\n",
       " 'amata',\n",
       " 'ami',\n",
       " 'andré',\n",
       " 'andy',\n",
       " 'angie',\n",
       " 'ann',\n",
       " 'anna',\n",
       " 'anthony',\n",
       " 'antonio',\n",
       " 'arizona',\n",
       " 'arkansas',\n",
       " 'armstrong',\n",
       " 'arrington',\n",
       " 'aumua',\n",
       " 'austin',\n",
       " 'axne',\n",
       " 'ayanna',\n",
       " 'b.',\n",
       " 'babin',\n",
       " 'bacon',\n",
       " 'baird',\n",
       " 'banks',\n",
       " 'barbara',\n",
       " 'barletta',\n",
       " 'barr',\n",
       " 'barragan',\n",
       " 'barragán',\n",
       " 'barry',\n",
       " 'barton',\n",
       " 'bass',\n",
       " 'beatty',\n",
       " 'ben',\n",
       " 'bennie',\n",
       " 'bera',\n",
       " 'bergman',\n",
       " 'bernice',\n",
       " 'beto',\n",
       " 'betty',\n",
       " 'beutler',\n",
       " 'beyer',\n",
       " 'beyer,',\n",
       " 'biggs',\n",
       " 'bilirakis',\n",
       " 'bill',\n",
       " 'bishop',\n",
       " 'bishop,',\n",
       " 'black',\n",
       " 'blackburn',\n",
       " 'blaine',\n",
       " 'blake',\n",
       " 'blum',\n",
       " 'blumenauer',\n",
       " 'blunt',\n",
       " 'bob',\n",
       " 'bobby',\n",
       " 'bonamici',\n",
       " 'bonnie',\n",
       " 'bordallo',\n",
       " 'bost',\n",
       " 'boyle',\n",
       " 'brad',\n",
       " 'bradley',\n",
       " 'brady',\n",
       " 'brenda',\n",
       " 'brendan',\n",
       " 'brett',\n",
       " 'brian',\n",
       " 'brindisi',\n",
       " 'brooks',\n",
       " 'brown',\n",
       " 'brownley',\n",
       " 'bruce',\n",
       " 'bryan',\n",
       " 'buck',\n",
       " 'bucshon',\n",
       " 'budd',\n",
       " 'buddy',\n",
       " 'burchett',\n",
       " 'burgess',\n",
       " 'bustos',\n",
       " 'butler',\n",
       " 'butterfield',\n",
       " 'byrne',\n",
       " 'c.',\n",
       " 'california',\n",
       " 'calvert',\n",
       " 'camacho',\n",
       " 'capuano',\n",
       " 'carbajal',\n",
       " 'cardenas',\n",
       " 'carlos',\n",
       " 'carol',\n",
       " 'carolina',\n",
       " 'carolyn',\n",
       " 'carson',\n",
       " 'carter',\n",
       " 'cartwright',\n",
       " 'case',\n",
       " 'casten',\n",
       " 'castor',\n",
       " 'castro',\n",
       " 'cathy',\n",
       " 'cedric',\n",
       " 'chabot',\n",
       " 'chaffetz',\n",
       " 'charles',\n",
       " 'charlie',\n",
       " 'chellie',\n",
       " 'cheney',\n",
       " 'cheri',\n",
       " 'chip',\n",
       " 'chris',\n",
       " 'chrissy',\n",
       " 'christopher',\n",
       " 'chu',\n",
       " 'chuck',\n",
       " 'chuy',\n",
       " 'cicilline',\n",
       " 'cisneros',\n",
       " 'cisneros,',\n",
       " 'clark',\n",
       " 'clarke',\n",
       " 'claudia',\n",
       " 'clay',\n",
       " 'cleaver',\n",
       " 'cline',\n",
       " 'cloud',\n",
       " 'clyburn',\n",
       " 'coffman',\n",
       " 'cohen',\n",
       " 'cole',\n",
       " 'coleman',\n",
       " 'colleen',\n",
       " 'collin',\n",
       " 'collins',\n",
       " 'colorado',\n",
       " 'comer',\n",
       " 'comstock',\n",
       " 'conaway',\n",
       " 'connecticut',\n",
       " 'connolly',\n",
       " 'conor',\n",
       " 'conyers',\n",
       " 'conyers,',\n",
       " 'cook',\n",
       " 'correa',\n",
       " 'costa',\n",
       " 'costello',\n",
       " 'courtney',\n",
       " 'cox',\n",
       " 'craig',\n",
       " 'crawford',\n",
       " 'crenshaw',\n",
       " 'crist',\n",
       " 'crow',\n",
       " 'crowley',\n",
       " 'cuellar',\n",
       " 'culberson',\n",
       " 'cummings',\n",
       " 'cunningham',\n",
       " 'curbelo',\n",
       " 'curtis',\n",
       " 'cynthia',\n",
       " 'cárdenas',\n",
       " 'd.',\n",
       " 'dakota',\n",
       " 'dan',\n",
       " 'dana',\n",
       " 'daniel',\n",
       " 'danny',\n",
       " 'darin',\n",
       " 'darrell',\n",
       " 'darren',\n",
       " 'dave',\n",
       " 'david',\n",
       " 'davidson',\n",
       " 'davis',\n",
       " 'dean',\n",
       " 'debbie',\n",
       " 'debra',\n",
       " 'defazio',\n",
       " 'degette',\n",
       " 'delauro',\n",
       " 'delbene',\n",
       " 'delgado',\n",
       " 'demings',\n",
       " 'denham',\n",
       " 'dennis',\n",
       " 'denny',\n",
       " 'denver',\n",
       " 'derek',\n",
       " 'desantis',\n",
       " 'desaulnier',\n",
       " 'desjarlais',\n",
       " 'deutch',\n",
       " 'devin',\n",
       " 'diana',\n",
       " 'diane',\n",
       " 'diaz',\n",
       " 'diaz-balart',\n",
       " 'dina',\n",
       " 'dingell',\n",
       " 'doggett',\n",
       " 'don',\n",
       " 'donald',\n",
       " 'donna',\n",
       " 'donovan',\n",
       " 'donovan,',\n",
       " 'doris',\n",
       " 'doug',\n",
       " 'doyle',\n",
       " 'drew',\n",
       " 'duffy',\n",
       " 'duncan',\n",
       " 'duncan,',\n",
       " 'dunn',\n",
       " 'dusty',\n",
       " 'dwight',\n",
       " 'e.',\n",
       " 'earl',\n",
       " 'ed',\n",
       " 'eddie',\n",
       " 'edward',\n",
       " 'elaine',\n",
       " 'eleanor',\n",
       " 'elijah',\n",
       " 'eliot',\n",
       " 'elise',\n",
       " 'elissa',\n",
       " 'elizabeth',\n",
       " 'ellis',\n",
       " 'ellison',\n",
       " 'emanuel',\n",
       " 'emmer',\n",
       " 'engel',\n",
       " 'eric',\n",
       " 'erik',\n",
       " 'escobar',\n",
       " 'eshoo',\n",
       " 'espaillat',\n",
       " 'estes',\n",
       " 'esty',\n",
       " 'eugene',\n",
       " 'evan',\n",
       " 'evans',\n",
       " 'ewell',\n",
       " 'f.',\n",
       " 'farenthold',\n",
       " 'faso',\n",
       " 'ferguson',\n",
       " 'filemon',\n",
       " 'fitzpatrick',\n",
       " 'fleischmann',\n",
       " 'fletcher',\n",
       " 'flores',\n",
       " 'florida',\n",
       " 'forrest',\n",
       " 'fortenberry',\n",
       " 'foster',\n",
       " 'foxx',\n",
       " 'francis',\n",
       " 'frank',\n",
       " 'frankel',\n",
       " 'franks',\n",
       " 'fred',\n",
       " 'frederica',\n",
       " 'frelinghuysen',\n",
       " 'french',\n",
       " 'fudge',\n",
       " 'fulcher',\n",
       " 'g.',\n",
       " 'g.k.',\n",
       " 'gabbard',\n",
       " 'gaetz',\n",
       " 'gallagher',\n",
       " 'gallego',\n",
       " 'garamendi',\n",
       " 'garcia',\n",
       " 'garcía',\n",
       " 'garland',\n",
       " 'garret',\n",
       " 'garrett',\n",
       " 'garrett,',\n",
       " 'gary',\n",
       " 'gay',\n",
       " 'gene',\n",
       " 'george',\n",
       " 'georgia',\n",
       " 'gerald',\n",
       " 'gianforte',\n",
       " 'gibbs',\n",
       " 'gilbert',\n",
       " 'glenn',\n",
       " 'gohmert',\n",
       " 'golden',\n",
       " 'gomez',\n",
       " 'gonzalez',\n",
       " 'gonzalez-colon',\n",
       " 'gonzález-colón',\n",
       " 'goodlatte',\n",
       " 'gosar',\n",
       " 'gottheimer',\n",
       " 'grace',\n",
       " 'granger',\n",
       " 'graves',\n",
       " 'green',\n",
       " 'greg',\n",
       " 'gregg',\n",
       " 'gregorio',\n",
       " 'gregory',\n",
       " 'griffith',\n",
       " 'grijalva',\n",
       " 'grisham',\n",
       " 'grothman',\n",
       " 'guest',\n",
       " 'gus',\n",
       " 'guthrie',\n",
       " 'gutierrez',\n",
       " 'gutiérrez',\n",
       " 'guy',\n",
       " 'gwen',\n",
       " 'h.',\n",
       " 'haaland',\n",
       " 'hakeem',\n",
       " 'hal',\n",
       " 'haley',\n",
       " 'hampshire',\n",
       " 'hanabusa',\n",
       " 'handel',\n",
       " 'hank',\n",
       " 'harder',\n",
       " 'harley',\n",
       " 'harold',\n",
       " 'harper',\n",
       " 'harris',\n",
       " 'hartzler',\n",
       " 'hastings',\n",
       " 'hayes',\n",
       " 'heck',\n",
       " 'henry',\n",
       " 'hensarling',\n",
       " 'hern',\n",
       " 'herrera',\n",
       " 'hice',\n",
       " 'higgins',\n",
       " 'hill',\n",
       " 'himes',\n",
       " 'holding',\n",
       " 'hollingsworth',\n",
       " 'holmes',\n",
       " 'horn',\n",
       " 'horsford',\n",
       " 'houlahan',\n",
       " 'hoyer',\n",
       " 'hudson',\n",
       " 'huffman',\n",
       " 'huizenga',\n",
       " 'hultgren',\n",
       " 'hunter',\n",
       " 'hurd',\n",
       " 'ii',\n",
       " 'iii',\n",
       " 'ileana',\n",
       " 'ilhan',\n",
       " 'illinois',\n",
       " 'indiana',\n",
       " 'iowa',\n",
       " 'issa',\n",
       " 'iv',\n",
       " 'j.',\n",
       " 'jack',\n",
       " 'jackie',\n",
       " 'jackson',\n",
       " 'jackson-lee',\n",
       " 'jacky',\n",
       " 'jahana',\n",
       " 'jaime',\n",
       " 'james',\n",
       " 'jamie',\n",
       " 'jan',\n",
       " 'janice',\n",
       " 'jared',\n",
       " 'jason',\n",
       " 'jayapal',\n",
       " 'jeb',\n",
       " 'jeff',\n",
       " 'jefferson',\n",
       " 'jeffries',\n",
       " 'jenkins',\n",
       " 'jennifer',\n",
       " 'jenniffer',\n",
       " 'jerrold',\n",
       " 'jerry',\n",
       " 'jersey',\n",
       " 'jesús',\n",
       " 'jim',\n",
       " 'jimmy',\n",
       " 'joaquin',\n",
       " 'jodey',\n",
       " 'jody',\n",
       " 'joe',\n",
       " 'john',\n",
       " 'johnson',\n",
       " 'johnson,',\n",
       " 'jordan',\n",
       " 'joseph',\n",
       " 'josh',\n",
       " 'josé',\n",
       " 'joyce',\n",
       " 'jr.',\n",
       " 'juan',\n",
       " 'judy',\n",
       " 'julia',\n",
       " 'justin',\n",
       " 'k.',\n",
       " 'kansas',\n",
       " 'kaptur',\n",
       " 'karen',\n",
       " 'katherine',\n",
       " 'kathleen',\n",
       " 'kathy',\n",
       " 'katie',\n",
       " 'katko',\n",
       " 'kay',\n",
       " 'keating',\n",
       " 'keith',\n",
       " 'keller',\n",
       " 'kelly',\n",
       " 'ken',\n",
       " 'kendra',\n",
       " 'kennedy',\n",
       " 'kenneth',\n",
       " 'kenny',\n",
       " 'kentucky',\n",
       " 'kevin',\n",
       " 'khanna',\n",
       " 'kihuen',\n",
       " 'kildee',\n",
       " 'kilili',\n",
       " 'kilmer',\n",
       " 'kim',\n",
       " 'kind',\n",
       " 'king',\n",
       " 'kinzinger',\n",
       " 'kirkpatrick',\n",
       " 'knight',\n",
       " 'krishnamoorthi',\n",
       " 'kristi',\n",
       " 'kurt',\n",
       " 'kuster',\n",
       " 'kustoff',\n",
       " 'kyrsten',\n",
       " 'l.',\n",
       " 'lacy',\n",
       " 'lahood',\n",
       " 'lamalfa',\n",
       " 'lamar',\n",
       " 'lamb',\n",
       " 'lamborn',\n",
       " 'lance',\n",
       " 'lane',\n",
       " 'langevin',\n",
       " 'larry',\n",
       " 'larsen',\n",
       " 'larson',\n",
       " 'latta',\n",
       " 'lauren',\n",
       " 'lawrence',\n",
       " 'lawson',\n",
       " 'lawson,',\n",
       " 'lee',\n",
       " 'leonard',\n",
       " 'lesko',\n",
       " 'levin',\n",
       " 'lewis',\n",
       " 'lieu',\n",
       " 'linda',\n",
       " 'linus',\n",
       " 'lipinski',\n",
       " 'lisa',\n",
       " 'liz',\n",
       " 'lizzie',\n",
       " 'lloyd',\n",
       " 'lofgren',\n",
       " 'lois',\n",
       " 'lori',\n",
       " 'lou',\n",
       " 'loudermilk',\n",
       " 'louie',\n",
       " 'louise',\n",
       " 'louisiana',\n",
       " 'love',\n",
       " 'lowenthal',\n",
       " 'lowey',\n",
       " 'lucas',\n",
       " 'lucille',\n",
       " 'lucy',\n",
       " 'luetkemeyer',\n",
       " 'luis',\n",
       " 'lujan',\n",
       " 'luján',\n",
       " 'luke',\n",
       " 'luria',\n",
       " 'lynch',\n",
       " 'lynn',\n",
       " 'm.',\n",
       " 'mac',\n",
       " 'madeleine',\n",
       " 'maine',\n",
       " 'malinowski',\n",
       " 'maloney',\n",
       " 'marc',\n",
       " 'marchant',\n",
       " 'marcia',\n",
       " 'marcy',\n",
       " 'marino',\n",
       " 'mario',\n",
       " 'mark',\n",
       " 'markwayne',\n",
       " 'marsha',\n",
       " 'marshall',\n",
       " 'martha',\n",
       " 'mary',\n",
       " 'maryland',\n",
       " 'massachusetts',\n",
       " 'massie',\n",
       " 'mast',\n",
       " 'matsui',\n",
       " 'matt',\n",
       " 'matthew',\n",
       " 'maurice',\n",
       " 'max',\n",
       " 'maxine',\n",
       " 'mcadams',\n",
       " 'mcbath',\n",
       " 'mccarthy',\n",
       " 'mccaul',\n",
       " 'mcclintock',\n",
       " 'mccollum',\n",
       " 'mceachin',\n",
       " 'mcgovern',\n",
       " 'mchenry',\n",
       " 'mcintosh',\n",
       " 'mckinley',\n",
       " 'mcmorris',\n",
       " 'mcnerney',\n",
       " 'mcsally',\n",
       " 'meadows',\n",
       " 'meeks',\n",
       " 'meng',\n",
       " 'messer',\n",
       " 'meuser',\n",
       " 'mexico',\n",
       " 'mia',\n",
       " 'michael',\n",
       " 'michelle',\n",
       " 'michigan',\n",
       " 'mike',\n",
       " 'mikie',\n",
       " 'miller',\n",
       " 'mimi',\n",
       " 'minnesota',\n",
       " 'miss',\n",
       " 'mississippi',\n",
       " 'missouri',\n",
       " 'mitchell',\n",
       " 'mo',\n",
       " 'mooney',\n",
       " 'moore',\n",
       " 'morelle',\n",
       " 'morgan',\n",
       " 'moulton',\n",
       " 'mr',\n",
       " 'mr.',\n",
       " 'mrs.',\n",
       " 'ms',\n",
       " 'ms.',\n",
       " 'mucarsel-powell',\n",
       " 'mullin',\n",
       " 'murphy',\n",
       " 'n.',\n",
       " 'nadler',\n",
       " 'nancy',\n",
       " 'nanette',\n",
       " 'napolitano',\n",
       " 'neal',\n",
       " 'nebraska',\n",
       " 'neguse',\n",
       " 'nevada',\n",
       " 'new',\n",
       " 'newhouse',\n",
       " 'nicolas',\n",
       " 'niki',\n",
       " 'nita',\n",
       " 'noem',\n",
       " 'nolan',\n",
       " 'norcross',\n",
       " 'norma',\n",
       " 'norman',\n",
       " 'north',\n",
       " 'norton',\n",
       " 'nunes',\n",
       " 'nydia',\n",
       " \"o'halleran\",\n",
       " \"o'rourke\",\n",
       " 'o.',\n",
       " 'ocasio-cortez',\n",
       " 'of',\n",
       " 'ohio',\n",
       " 'oklahoma',\n",
       " 'olson',\n",
       " 'omar',\n",
       " 'o’halleran',\n",
       " 'o’rourke',\n",
       " 'p.',\n",
       " 'palazzo',\n",
       " 'pallone',\n",
       " 'pallone,',\n",
       " 'palmer',\n",
       " 'panetta',\n",
       " 'pappas',\n",
       " 'pascrell',\n",
       " 'pascrell,',\n",
       " 'pat',\n",
       " 'patrick',\n",
       " 'paul',\n",
       " 'paulsen',\n",
       " 'payne',\n",
       " 'payne,',\n",
       " 'pearce',\n",
       " 'pelosi',\n",
       " 'pence',\n",
       " 'pennsylvania',\n",
       " 'perlmutter',\n",
       " 'perry',\n",
       " 'pete',\n",
       " 'peter',\n",
       " 'peters',\n",
       " 'peterson',\n",
       " 'phil',\n",
       " 'phillips',\n",
       " 'pingree',\n",
       " 'pittenger',\n",
       " 'plaskett',\n",
       " 'pocan',\n",
       " 'poe',\n",
       " 'poliquin',\n",
       " 'polis',\n",
       " 'porter',\n",
       " 'posey',\n",
       " 'pramila',\n",
       " 'pressley',\n",
       " 'price',\n",
       " 'puerto',\n",
       " 'q.',\n",
       " 'quigley',\n",
       " 'r.',\n",
       " 'radewagen',\n",
       " 'raja',\n",
       " 'ralph',\n",
       " 'randy',\n",
       " 'rashida',\n",
       " 'raskin',\n",
       " 'ratcliffe',\n",
       " 'raul',\n",
       " 'ray',\n",
       " 'raúl',\n",
       " 'reed',\n",
       " 'reichert',\n",
       " 'reschenthaler',\n",
       " 'rice',\n",
       " 'richard',\n",
       " 'richmond',\n",
       " 'rick',\n",
       " 'rico',\n",
       " 'riggleman',\n",
       " 'ro',\n",
       " 'rob',\n",
       " 'robert',\n",
       " 'robin',\n",
       " 'roby',\n",
       " 'rochester',\n",
       " 'rod',\n",
       " 'rodgers',\n",
       " 'rodney',\n",
       " 'roe',\n",
       " 'roger',\n",
       " 'rogers',\n",
       " 'rohrabacher',\n",
       " 'rokita',\n",
       " 'ron',\n",
       " 'rooney',\n",
       " 'ros-lehtinen',\n",
       " 'rosa',\n",
       " 'rose',\n",
       " 'rosen',\n",
       " 'roskam',\n",
       " 'ross',\n",
       " 'rothfus',\n",
       " 'rouda',\n",
       " 'rouzer',\n",
       " 'roy',\n",
       " 'roybal-allard',\n",
       " 'royce',\n",
       " 'ruben',\n",
       " 'ruiz',\n",
       " 'rush',\n",
       " 'russ',\n",
       " 'russell',\n",
       " 'rutherford',\n",
       " 'ryan',\n",
       " 's.',\n",
       " 'sablan',\n",
       " 'salud',\n",
       " 'sam',\n",
       " 'san',\n",
       " 'sanchez',\n",
       " 'sander',\n",
       " 'sanford',\n",
       " 'sarbanes',\n",
       " 'scalise',\n",
       " 'scanlon',\n",
       " 'schakowsky',\n",
       " 'schiff',\n",
       " 'schneider',\n",
       " 'schrader',\n",
       " 'schrier',\n",
       " 'schultz',\n",
       " 'schweikert',\n",
       " 'scott',\n",
       " 'sean',\n",
       " 'sensenbrenner',\n",
       " 'sensenbrenner,',\n",
       " 'serrano',\n",
       " 'sessions',\n",
       " 'seth',\n",
       " 'sewell',\n",
       " 'shalala',\n",
       " 'shea-porter',\n",
       " 'sheila',\n",
       " 'sherman',\n",
       " 'sherrill',\n",
       " 'shimkus',\n",
       " 'shuster',\n",
       " 'simpson',\n",
       " 'sinema',\n",
       " 'sires',\n",
       " 'slaughter',\n",
       " 'slotkin',\n",
       " 'small',\n",
       " 'smith',\n",
       " 'smucker',\n",
       " 'soto',\n",
       " 'south',\n",
       " 'spanberger',\n",
       " 'spano',\n",
       " 'speier',\n",
       " 'sr.',\n",
       " 'stacey',\n",
       " 'stanton',\n",
       " 'stauber',\n",
       " 'stefanik',\n",
       " 'steil',\n",
       " 'steny',\n",
       " 'stephanie',\n",
       " 'stephen',\n",
       " 'steube',\n",
       " 'stevan',\n",
       " 'steve',\n",
       " 'steven',\n",
       " 'stevens',\n",
       " 'stewart',\n",
       " 'stivers',\n",
       " 'suozzi',\n",
       " 'susan',\n",
       " 'susie',\n",
       " 'suzan',\n",
       " 'suzanne',\n",
       " 'swalwell',\n",
       " 'sylvia',\n",
       " 'sánchez',\n",
       " 't.',\n",
       " 'takano',\n",
       " 'taylor',\n",
       " 'ted',\n",
       " 'tennessee',\n",
       " 'tenney',\n",
       " 'terri',\n",
       " 'texas',\n",
       " 'theodore',\n",
       " 'thomas',\n",
       " 'thompson',\n",
       " 'thornberry',\n",
       " 'tiberi',\n",
       " 'tim',\n",
       " 'timothy',\n",
       " 'tipton',\n",
       " 'titus',\n",
       " 'tj',\n",
       " 'tlaib',\n",
       " 'todd',\n",
       " 'tom',\n",
       " 'tonko',\n",
       " 'tony',\n",
       " 'torres',\n",
       " 'trahan',\n",
       " 'trent',\n",
       " 'trey',\n",
       " 'trone',\n",
       " 'trott',\n",
       " 'tsongas',\n",
       " 'tulsi',\n",
       " 'turner',\n",
       " 'underwood',\n",
       " 'upton',\n",
       " 'utah',\n",
       " 'v.',\n",
       " 'val',\n",
       " 'van',\n",
       " 'vargas',\n",
       " 'veasey',\n",
       " 'vela',\n",
       " 'velazquez',\n",
       " 'velázquez',\n",
       " 'veronica',\n",
       " 'vicente',\n",
       " 'vicky',\n",
       " 'virginia',\n",
       " 'visclosky',\n",
       " 'w.',\n",
       " 'wagner',\n",
       " 'walberg',\n",
       " 'walden',\n",
       " 'walker',\n",
       " 'walorski',\n",
       " 'walters',\n",
       " 'waltz',\n",
       " 'walz',\n",
       " 'warren',\n",
       " 'washington',\n",
       " 'wasserman',\n",
       " 'waters',\n",
       " 'watkins',\n",
       " 'watson',\n",
       " 'weber',\n",
       " 'weber,',\n",
       " 'webster',\n",
       " 'welch',\n",
       " 'wenstrup',\n",
       " 'west',\n",
       " 'westerman',\n",
       " 'wexton',\n",
       " 'wild',\n",
       " 'will',\n",
       " 'william',\n",
       " 'williams',\n",
       " 'wilson',\n",
       " 'wisconsin',\n",
       " 'wittman',\n",
       " 'wm.',\n",
       " 'womack',\n",
       " 'woodall',\n",
       " 'wright',\n",
       " 'xochitl',\n",
       " 'yarmuth',\n",
       " 'yoder',\n",
       " 'yoho',\n",
       " 'york',\n",
       " 'young',\n",
       " 'yvette',\n",
       " 'z.',\n",
       " 'zeldin',\n",
       " 'zoe'}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "63378421",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "617\n"
     ]
    }
   ],
   "source": [
    "#also add in the stopwords list used by TBIP paper authors to preprocess senate speeches data - it consists\n",
    "#all state names, cities, month names, days of week, and other stopwords/procedural terms - very useful. \n",
    "stopwords_from_senate_speeches_tbip = open('../../setup/stopwords/senate_speeches.txt').readlines()\n",
    "stopwords_from_senate_speeches_tbip = list(map(lambda x:x.rstrip(), stopwords_from_senate_speeches_tbip))\n",
    "print(len(stopwords_from_senate_speeches_tbip))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fbb9a07d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1456\n"
     ]
    }
   ],
   "source": [
    "stopwords = stopwords.union(set(stopwords_from_senate_speeches_tbip))\n",
    "print(len(stopwords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "27c53d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('stopwords.txt', 'w')\n",
    "for i, x in enumerate(list(stopwords)):\n",
    "    f.write(x)\n",
    "    if i < len(stopwords) - 1:\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7ae73420",
   "metadata": {},
   "outputs": [],
   "source": [
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "396bfa30",
   "metadata": {},
   "outputs": [],
   "source": [
    "#while it is possible to add more jargon terms perhaps, do not want to overdo stopwords, because words can be \n",
    "#highly contextual and have meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "174ce607",
   "metadata": {},
   "outputs": [],
   "source": [
    "#rest of the preprocessing is using the script provided in the TBIP repo - setup/senate_speeches_to_bag_of_words.py \n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import sparse\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "053b0670",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85173\n",
      "85173\n"
     ]
    }
   ],
   "source": [
    "speakers = list(raw_data['Speaker_Bioguide_ID'])\n",
    "print(len(speakers))\n",
    "speeches = list(raw_data['Text'])\n",
    "print(len(speeches))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "2b873edd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85173\n",
      "513\n"
     ]
    }
   ],
   "source": [
    "speaker_to_speaker_id = dict(\n",
    "    [(y, x) for x, y in enumerate(sorted(set(speakers)))])\n",
    "author_indices = np.array(\n",
    "    [speaker_to_speaker_id[s] for s in speakers])\n",
    "print(len(author_indices))\n",
    "author_map = np.array(list(speaker_to_speaker_id.keys()))\n",
    "print(len(author_map))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "76fd924f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85173\n"
     ]
    }
   ],
   "source": [
    "print(len(speeches))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "c6f12345",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/.conda/envs/tbip/lib/python3.6/site-packages/sklearn/feature_extraction/text.py:301: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['allard', 'andr', 'balart', 'barrag', 'col', 'colon', 'cortez', 'garc', 'gonz', 'guti', 'halleran', 'jes', 'jos', 'jr', 'lehtinen', 'lez', 'luj', 'mucarsel', 'nchez', 'ocasio', 'powell', 'ra', 'rdenas', 'ros', 'rourke', 'roybal', 'rrez', 'shea', 'sr', 'vel', 'wm', 'zquez'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    }
   ],
   "source": [
    "count_vectorizer = CountVectorizer(min_df=0.001,\n",
    "                                   max_df=0.75, \n",
    "                                   stop_words=stopwords, \n",
    "                                   ngram_range=(1, 3),\n",
    "                                   token_pattern=\"[a-zA-Z]+\")\n",
    "# Learn initial document term matrix. This is only initial because we use it to\n",
    "# identify words to exclude based on author counts.\n",
    "counts = count_vectorizer.fit_transform(speeches)\n",
    "vocabulary = np.array(\n",
    "    [k for (k, v) in sorted(count_vectorizer.vocabulary_.items(), \n",
    "                            key=lambda kv: kv[1])])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "e7cc8469",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(85173, 10588)\n",
      "10588\n"
     ]
    }
   ],
   "source": [
    "print(counts.shape)\n",
    "print(len(vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "d5178de2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 513/513 [00:16<00:00, 30.23it/s]\n"
     ]
    }
   ],
   "source": [
    "author_to_inds = {}\n",
    "for a in tqdm(list(author_map)):\n",
    "    inds = []\n",
    "    author_ind = speaker_to_speaker_id[a]\n",
    "    for i, ind in enumerate(list(author_indices)):\n",
    "        if ind==author_ind:\n",
    "            inds.append(i)\n",
    "    author_to_inds[a] = inds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "da8ef8b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_per_author_counts(counts, author_to_inds):\n",
    "    list_of_arrays = []\n",
    "    for a in author_to_inds:\n",
    "        inds = author_to_inds[a]\n",
    "        list_of_arrays.append(np.array(np.sum(counts[inds], 0)))#.reshape((1, counts.shape[1])))\n",
    "    return np.concatenate(list_of_arrays, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "cd824625",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(513, 10588)\n"
     ]
    }
   ],
   "source": [
    "# Remove phrases spoken by less than 50 representatives\n",
    "min_authors_per_word = 50\n",
    "counts_per_author = get_per_author_counts(counts, author_to_inds)\n",
    "print(counts_per_author.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "356f308a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10151\n"
     ]
    }
   ],
   "source": [
    "acceptable_words = []\n",
    "for i in range(len(vocabulary)):\n",
    "    if np.count_nonzero(counts_per_author[:, i]) >= min_authors_per_word:\n",
    "        acceptable_words.append(i)\n",
    "print(len(acceptable_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "fc115984",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer = CountVectorizer(ngram_range=(1, 3),\n",
    "                                   vocabulary=vocabulary[acceptable_words])\n",
    "counts = count_vectorizer.fit_transform(speeches)\n",
    "vocabulary = np.array(\n",
    "    [k for (k, v) in sorted(count_vectorizer.vocabulary_.items(), \n",
    "                            key=lambda kv: kv[1])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "c20f97af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(85173, 10151)\n",
      "10151\n"
     ]
    }
   ],
   "source": [
    "print(counts.shape)\n",
    "print(len(vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "6d7b3a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# `n_gram_to_unigram` takes as key an index to an n-gram in the vocabulary\n",
    "# and its value is a list of the vocabulary indices of the corresponding \n",
    "# unigrams.\n",
    "n_gram_indices = np.where(\n",
    "  np.array([len(word.split(' ')) for word in vocabulary]) > 1)[0]\n",
    "n_gram_to_unigrams = {}\n",
    "for n_gram_index in n_gram_indices:\n",
    "    matching_unigrams = []\n",
    "    for unigram in vocabulary[n_gram_index].split(' '):\n",
    "        if unigram in vocabulary:\n",
    "            matching_unigrams.append(np.where(vocabulary == unigram)[0][0])\n",
    "    n_gram_to_unigrams[n_gram_index] = matching_unigrams\n",
    "\n",
    "# `n_grams_to_bigrams` now breaks apart trigrams and higher to find bigrams \n",
    "# as subsets of these words.\n",
    "n_grams_to_bigrams = {}\n",
    "for n_gram_index in n_gram_indices:\n",
    "    split_n_gram = vocabulary[n_gram_index].split(' ')\n",
    "    n_gram_length = len(split_n_gram) \n",
    "    if n_gram_length > 2:\n",
    "        bigram_matches = []\n",
    "        for i in range(0, n_gram_length - 1):\n",
    "            bigram = \" \".join(split_n_gram[i:(i + 2)])\n",
    "            if bigram in vocabulary:\n",
    "                bigram_matches.append(np.where(vocabulary == bigram)[0][0])\n",
    "        n_grams_to_bigrams[n_gram_index] = bigram_matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "6a40b1ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 85173/85173 [04:08<00:00, 342.53it/s]\n"
     ]
    }
   ],
   "source": [
    "# Go through counts, and remove a unigram each time a bigram superset \n",
    "# appears. Also remove a bigram each time a trigram superset appears.\n",
    "# Note this isn't perfect: if bigrams overlap (e.g. \"global health care\" \n",
    "# contains \"global health\" and \"health care\"), we count them both. This\n",
    "# may introduce a problem where we subract a unigram count twice, so we also\n",
    "# ensure non-negativity.\n",
    "#counts_dense = counts.toarray()\n",
    "for i in tqdm(range(counts.shape[0])):\n",
    "    n_grams_in_doc = np.where(counts[i, n_gram_indices].toarray() > 0)[0]\n",
    "    sub_n_grams = n_gram_indices[n_grams_in_doc]\n",
    "    for n_gram in sub_n_grams:\n",
    "        counts[i, n_gram_to_unigrams[n_gram]] = sparse.csr_matrix(counts[i, n_gram_to_unigrams[n_gram]].toarray() - counts[i, n_gram])\n",
    "        if n_gram in n_grams_to_bigrams:\n",
    "            counts[i, n_grams_to_bigrams[n_gram]] = sparse.csr_matrix(counts[i, n_grams_to_bigrams[n_gram]].toarray() - counts[i, n_gram])\n",
    "counts[counts < 0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "f437ac32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(85173, 10151)\n"
     ]
    }
   ],
   "source": [
    "print(counts.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "bd1a3754",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 85173/85173 [00:22<00:00, 3744.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(84063, 10151)\n",
      "(84063,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Remove speeches with no words.\n",
    "existing_speeches = []#np.where(np.sum(counts_dense, axis=1) > 0)[0]\n",
    "for i in tqdm(range(counts.shape[0])):\n",
    "    if counts[i].sum() > 0:\n",
    "        existing_speeches.append(i)\n",
    "counts = counts[existing_speeches]\n",
    "print(counts.shape)\n",
    "author_indices = author_indices[existing_speeches]\n",
    "print(author_indices.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "7adda18b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save data.\n",
    "\n",
    "# `counts.npz` is a [num_documents, num_words] sparse matrix containing the\n",
    "# word counts for each document.\n",
    "sparse.save_npz(\"clean/counts.npz\",\n",
    "                counts.astype(np.float32))\n",
    "\n",
    "# `author_indices.npy` is a [num_documents] vector where each entry is an\n",
    "# integer indicating the author of the corresponding document.\n",
    "np.save(\"clean/author_indices.npy\", author_indices)\n",
    "\n",
    "# `vocabulary.txt` is a [num_words] vector where each entry is a string\n",
    "# denoting the corresponding word in the vocabulary.\n",
    "np.savetxt(\"clean/vocabulary.txt\", vocabulary, fmt=\"%s\")\n",
    "\n",
    "# `author_map.txt` is a [num_authors] vector of strings providing the bioguide ID of\n",
    "# each author in the corpus.\n",
    "np.savetxt(\"clean/author_map.txt\", author_map, fmt=\"%s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "ede09272",
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-123-aa4add81823c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m np.savetxt(\"clean/raw_documents.txt\", \n\u001b[1;32m      5\u001b[0m            \u001b[0mraw_documents\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m            fmt=\"%s\")\n\u001b[0m",
      "\u001b[0;32m/workspace/.conda/envs/tbip/lib/python3.6/site-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36msavetxt\u001b[0;34m(fname, X, fmt, delimiter, newline, header, footer, comments, encoding)\u001b[0m\n\u001b[1;32m   1370\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1371\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1372\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1373\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1374\u001b[0m         \u001b[0;31m# Handle 1-dimensional arrays\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/workspace/.conda/envs/tbip/lib/python3.6/site-packages/numpy/core/numeric.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m    536\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    537\u001b[0m     \"\"\"\n\u001b[0;32m--> 538\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    539\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# `raw_documents.txt` contains all the documents we ended up using.\n",
    "raw_documents = [document.replace(\"\\n\", ' ').replace(\"\\r\", ' ') \n",
    "                 for i, document in enumerate(speeches) if i in existing_speeches]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "488ba607",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84063\n"
     ]
    }
   ],
   "source": [
    "print(len(raw_documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "2629ac9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('clean/raw_documents.txt', 'w')\n",
    "for i, doc in enumerate(raw_documents):\n",
    "    f.write(doc)\n",
    "    if i < len(raw_documents) - 1:\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "56754e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "894bd1de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "84063"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(existing_speeches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "41a54f8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 84063 entries, 0 to 85975\n",
      "Data columns (total 5 columns):\n",
      "Speaker_Bioguide_ID    84063 non-null object\n",
      "Speaker_Name           84063 non-null object\n",
      "Text                   84063 non-null object\n",
      "Date                   84063 non-null object\n",
      "Legislative Body       84063 non-null object\n",
      "dtypes: object(5)\n",
      "memory usage: 3.8+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "raw_data = raw_data.iloc[existing_speeches]\n",
    "print(raw_data.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "a18bc991",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data.to_csv('finalized_tbip_speech_set_raw_original_data_floor_speeches_house.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e8acf0f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
